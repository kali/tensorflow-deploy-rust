// vim: ft=arm

// C tile regs
// 
//      q8[0]    q10[0]   q12[0]    q14[0]
//      q8[1]    q10[1]   q12[1]    q14[1]
//      q8[2]    q10[2]   q12[2]    q14[2]
//      q8[3]    q10[3]   q12[3]    q14[3]
//
//      q9[0]    q11[0]   q13[0]    q15[0]
//      q9[1]    q11[1]   q13[1]    q15[1]
//      q9[2]    q11[2]   q13[2]    q15[2]
//      q9[3]    q11[3]   q13[3]    q15[3]

// packed A buffering (2x8 values): alternating q0, q1 with q2, q3
// packed B buffering (2x4 values): alternating q4 with q5

// q6 and q7 are left alone -> no need to preserve s24-s31

    .arm
    .text
    .global neon_stile8x4
    .type neon_stile8x4, %function

neon_stile8x4:

    push        { r4-r10 }               // no lr (we're a leaf), no fp. #24 bytes
    vpush       { q4-q5 }

    veor      q8, q8 ,q8
    veor      q9, q9 ,q9
    veor      q10, q10 ,q10
    veor      q11, q11 ,q11
    veor      q12, q12 ,q12
    veor      q13, q13 ,q13
    veor      q14, q14 ,q14
    veor      q15, q15 ,q15

    ldm     r0, { r7, r8, r9, r10 }      // a, b, c, lin
    // check a->discriminant == 1 (packed)
    ldm     r7, { r1, r2 }
    cmp     r1, #1
    bne     .unsupported
    mov     r1, r2 // packed A ptr
    pld     [r1]

    // check linear
    ldm     r10, {r5, r6}
    cmp     r5, #0
    bne     .unsupported
    cmp     r6, #0
    beq     .non_linear

    mov     r3, r6 // k

    // B
    ldm     r8, { r7, r8, r9 }
    cmp     r7, #1
    beq     .packed_packed
    cmp     r7, #2
    beq     .packed_tops_and_offsets
    b       .unsupported

    .packed_tops_and_offsets:
    mov             r2, r8                  // rows offsets
    ldm             r9, {r5, r6, r7, r8}    // cols tops ptr

    .packed_tops_and_offsets_loop_1:

    vldmia          r1!, { q0, q1 }
    ldr             r4, [r2], #4

    add             r9, r5, r4
    add             r10, r6, r4
    vldr            s16, [r9]
    vldr            s17, [r10]

    add             r9, r7, r4
    add             r10, r8, r4
    vldr            s18, [r9]
    vldr            s19, [r10]

    vmla.f32        q8, q0, d8[0]
    vmla.f32        q9, q1, d8[0]

    vmla.f32        q10, q0, d8[1]
    vmla.f32        q11, q1, d8[1]

    vmla.f32        q12, q0, d9[0]
    vmla.f32        q13, q1, d9[0]

    vmla.f32        q14, q0, d9[1]
    vmla.f32        q15, q1, d9[1]

    subs            r3, r3, #1
    bne .packed_tops_and_offsets_loop_1
    b   .non_linear

    .packed_packed:
    mov     r2, r8                          // packed B ptr
    pld     [r2]

    cmp r3, #4
    blt .packed_packed_loop_1

    .packed_packed_loop_4:

    // 1
    vldmia          r1!, { q0, q1 }
    vldmia          r2!, { q4 }

    vmla.f32        q8, q0, d8[0]
    vmla.f32        q9, q1, d8[0]

    vldmia          r1!, { q2, q3 }

    vmla.f32        q10, q0, d8[1]
    vmla.f32        q11, q1, d8[1]

    vldmia          r2!, { q5 }

    vmla.f32        q12, q0, d9[0]
    vmla.f32        q13, q1, d9[0]

    vmla.f32        q14, q0, d9[1]
    vmla.f32        q15, q1, d9[1]

    // 2
    vldmia          r1!, { q0, q1 }

    vmla.f32        q8, q2, d10[0]
    vmla.f32        q9, q3, d10[0]

    vldmia          r2!, { q4 }

    vmla.f32        q10, q2, d10[1]
    vmla.f32        q11, q3, d10[1]

    vmla.f32        q12, q2, d11[0]
    vmla.f32        q13, q3, d11[0]

    vmla.f32        q14, q2, d11[1]
    vmla.f32        q15, q3, d11[1]

    // 3
    vldmia          r1!, { q2, q3 }

    vmla.f32        q8, q0, d8[0]
    vmla.f32        q9, q1, d8[0]

    vldmia          r2!, { q5 }

    vmla.f32        q10, q0, d8[1]
    vmla.f32        q11, q1, d8[1]

    pld [r1]

    vmla.f32        q12, q0, d9[0]
    vmla.f32        q13, q1, d9[0]

    pld [r1, #8]

    vmla.f32        q14, q0, d9[1]
    vmla.f32        q15, q1, d9[1]
    pld [r1, #16]

    // 4

    vmla.f32        q8, q2, d10[0]
    vmla.f32        q9, q3, d10[0]

    pld [r1, #24]

    vmla.f32        q10, q2, d10[1]
    vmla.f32        q11, q3, d10[1]

    pld [r2]

    vmla.f32        q12, q2, d11[0]
    vmla.f32        q13, q3, d11[0]

    pld [r2, #8]

    vmla.f32        q14, q2, d11[1]
    vmla.f32        q15, q3, d11[1]

    sub r3, r3, #4
    cmp r3, #4
    bge .packed_packed_loop_4

    cmp r3, #0
    beq .non_linear

    .packed_packed_loop_1:

    vldmia          r1!, { q0, q1 }
    vldmia          r2!, { q4 }

    vmla.f32        q8, q0, d8[0]
    vmla.f32        q9, q1, d8[0]

    vmla.f32        q10, q0, d8[1]
    vmla.f32        q11, q1, d8[1]

    vmla.f32        q12, q0, d9[0]
    vmla.f32        q13, q1, d9[0]

    vmla.f32        q14, q0, d9[1]
    vmla.f32        q15, q1, d9[1]

    subs r3, r3, #1
    bne .packed_packed_loop_1
    b  .non_linear

    .non_linear:

    ldr     r1, [r0, #16]
    cmp     r1, #0
    beq     .STORE
    sub     r1, #8

    .non_linear_loop:
    add     r1, #8
    ldr     r2, [r1]
    cmp     r2, #0
    beq     .STORE
    cmp     r2, #3
    beq     .non_linear_addc

    b .unsupported

    .non_linear_addc:

    ldr     r3, [r0, #8]
    ldr     r4, [r3]
    cmp     r4, #0
    bne     .unsupported

    ldr     r4, [r3, #4]   // C ptr
    ldr     r5, [r3, #8]   // rsc
    ldr     r6, [r3, #12]  // csc

    {% for col in (0..3) %}
        mov         r7, r4
        {% for reg in (0..3) %}
            vld1.f32    d0[0], [ r7 ]
            add         r7, r7, r5
            vld1.f32    d0[1], [ r7 ]
            add         r7, r7, r5
            vadd.f32    d{{col | times: 4 | plus: reg | plus : 16}}, d0
        {% endfor %}
        add r4, r4, r6
    {% endfor %}

    .STORE:

    ldr     r3, [r0, #8]
    ldm     r3, { r4, r5, r6, r7 } // discr, ...
    cmp     r4, #0
    bne     .unsupported

    // r5,r6,r7 are c,src,csc

    {% for col in (0..3) %}
        mov         r8, r5
        {% for reg in (0..3) %}
            vst1.f32    d{{col | times: 4 | plus: reg | plus : 16}}[0], [ r8 ]
            add         r8, r8, r6
            vst1.f32    d{{col | times: 4 | plus: reg | plus : 16}}[1], [ r8 ]
            {% if reg < 3 %}
                add         r8, r8, r6
            {% endif %}
        {% endfor %}
        {% if col < 3 %}
            add r5, r5, r7
        {% endif %}
    {% endfor %}

    mov         r0,     #0

.return:

    vpop        { q4-q5 }
    pop         { r4-r10 }

    bx          lr

.unsupported:
    mov         r0,     #1
    b           .return
