// vim: ft=arm

// C tile regs
// 
//      q8[0] 
//      q8[1] 
//      q8[2] 
//      q8[3] 
//    
//      ....
//
//      q15[0]
//      q15[1]
//      q15[2]
//      q15[3]

    .arm
    .text
    .global armv7neon_mmm_f32_32x1_{{sufix}}
    .type armv7neon_mmm_f32_32x1_{{suffix}}, %function

armv7neon_mmm_f32_32x1_{{suffix}}:

    pld     [r0]
    push    { r4-r12 }
    vpush   { q4-q7 }

    veor    q8, q8 ,q8
    veor    q9, q9 ,q9
    veor    q10, q10 ,q10
    veor    q11, q11 ,q11
    veor    q12, q12 ,q12
    veor    q13, q13 ,q13
    veor    q14, q14 ,q14
    veor    q15, q15 ,q15

    ldm     r0, { r7, r8, r9, r10 }      // a, b, c, lin
    ldm     r7, { r1, r2 }
    pld     [r10]
    pld     [r8]
    pld     [r9]
    // check a->discriminant == 1 (packed)
    cmp     r1, #1
    bne     .unsupported
    mov     r1, r2                       // packed A ptr

    // check linear
    ldm     r10, {r5, r6}
    pld     [r1]
    pld     [r1, #32]
    pld     [r1, #64]
    pld     [r1, #96]
    cmp     r5, #0
    bne     .unsupported
    cmp     r6, #0
    beq     .non_linear

    mov     r3, r6 // k

    // B
    ldm     r8, { r4, r5, r6 }
    pld     [r1, #128]
    pld     [r1, #160]
    pld     [r1, #192]
    pld     [r1, #224]
    pld     [r5]
    cmp     r4, #1
    beq     .packed_packed
    cmp     r4, #2
    beq     .packed_tops_and_offsets
    cmp     r4, #3
    beq     .packed_strides_vec
    b       .unsupported

    .packed_tops_and_offsets:
    // r5: row byte offset, r6: col_ptrs
    ldr             r6, [r6]                // r6: col top ptr

    .packed_tops_and_offsets_loop_1:
    ldr             r2, [ r5 ], #4

    add             r7, r6, r2
    vldr            s16, [r7]

    vldmia          r1!, { q0-q3 }

    vmla.f32        q8, q0, d8[0]
    vmla.f32        q9, q1, d8[0]

    vldmia          r1!, { q0-q1 }

    vmla.f32        q10, q2, d8[0]
    vmla.f32        q11, q3, d8[0]

    vldmia          r1!, { q2-q3 }
    vmla.f32        q12, q0, d8[0]
    vmla.f32        q13, q1, d8[0]

    vmla.f32        q14, q2, d8[0]
    vmla.f32        q15, q3, d8[0]

    subs            r3, r3, #1
    bne .packed_tops_and_offsets_loop_1

    .packed_tops_and_offsets_end:
    b   .non_linear

    .packed_packed:
    pld     [r5]                           // packed B ptr       

/*
    cmp r3, #4
    blt .packed_packed_loop_1

    .packed_packed_loop_4:

    // 1
    pld             [r1, #128]
    vldmia          r1!, { q0, q1 }
    pld             [r5, #128]
    vldmia          r5!, { q4 }

    vmla.f32        q8, q0, d8[0]
    vmla.f32        q9, q1, d8[0]

    pld             [r1, #128]
    vldmia          r1!, { q2, q3 }

    vmla.f32        q10, q0, d8[1]
    vmla.f32        q11, q1, d8[1]

    pld             [r5, #128]
    vldmia          r5!, { q5 }

    vmla.f32        q12, q0, d9[0]
    vmla.f32        q13, q1, d9[0]

    vmla.f32        q14, q0, d9[1]
    vmla.f32        q15, q1, d9[1]

    // 2
    pld             [r1, #128]
    vldmia          r1!, { q0, q1 }

    vmla.f32        q8, q2, d10[0]
    vmla.f32        q9, q3, d10[0]

    vldmia          r5!, { q4 }

    vmla.f32        q10, q2, d10[1]
    vmla.f32        q11, q3, d10[1]

    vmla.f32        q12, q2, d11[0]
    vmla.f32        q13, q3, d11[0]

    vmla.f32        q14, q2, d11[1]
    vmla.f32        q15, q3, d11[1]

    // 3
    pld             [r1, #128]
    vldmia          r1!, { q2, q3 }

    vmla.f32        q8, q0, d8[0]
    vmla.f32        q9, q1, d8[0]

    pld             [r5, #128]
    vldmia          r5!, { q5 }

    vmla.f32        q10, q0, d8[1]
    vmla.f32        q11, q1, d8[1]

    vmla.f32        q12, q0, d9[0]
    vmla.f32        q13, q1, d9[0]

    vmla.f32        q14, q0, d9[1]
    vmla.f32        q15, q1, d9[1]

    // 4
    vmla.f32        q8, q2, d10[0]
    vmla.f32        q9, q3, d10[0]

    vmla.f32        q10, q2, d10[1]
    vmla.f32        q11, q3, d10[1]

    vmla.f32        q12, q2, d11[0]
    vmla.f32        q13, q3, d11[0]

    vmla.f32        q14, q2, d11[1]
    vmla.f32        q15, q3, d11[1]

    sub r3, r3, #4
    cmp r3, #4
    bge .packed_packed_loop_4
    */

    cmp r3, #0
    beq .non_linear

    .packed_packed_loop_1:

    vldmia          r1!, { q0-q3 }
    vld1.32         { d8[0] }, [r5]!

    vmla.f32        q8, q0, d8[0]
    vmla.f32        q9, q1, d8[0]

    vldmia          r1!, { q0-q1 }

    vmla.f32        q10, q2, d8[0]
    vmla.f32        q11, q3, d8[0]

    vldmia          r1!, { q2-q3 }
    vmla.f32        q12, q0, d8[0]
    vmla.f32        q13, q1, d8[0]

    vmla.f32        q14, q2, d8[0]
    vmla.f32        q15, q3, d8[0]

    subs r3, r3, #1
    bne .packed_packed_loop_1
    b   .non_linear

    .packed_strides_vec:
    // r5 -> b ptr, r6 -> b stride
/*
    cmp r3, #4
    blt .packed_strides_vec_loop_1

    cmp r6, #4
    bne .packed_strides_vec_loop_4

    .packed_strides_vec_loop_4_contig:
    // loop read 8 rows * 4 bytes * 4 k = 128 bytes from A, 8 q regs
    // loop read 1 cols * 4 bytes * 4 k = 16 bytes from B
    // cache entry is 32bytes <=> PLD preloads 2 q registers

    pld             [r1, #256]
    vld1.32         { q0, q1 }, [r1]!

    pld             [r5, #32]
    vld1.32         { q4 }, [r5]!

    pld             [r1, #256]
    vld1.32         { q2, q3 }, [r1]!

    vmla.f32        q8, q0, d8[0]
    pld             [r1, #256]
    vld1.32         { q0 }, [r1]!
    vmla.f32        q9, q1, d8[0]

    vld1.32         { q1 }, [r1]!

    vmla.f32        q10, q2, d8[1]
    pld             [r1, #256]
    vld1.32         { q2 }, [r1]!
    vmla.f32        q11, q3, d8[1]
    vld1.32         { q3 }, [r1]!

    vmla.f32        q8, q0, d9[0]
    vmla.f32        q9, q1, d9[0]

    vmla.f32        q10, q2, d9[1]
    vmla.f32        q11, q3, d9[1]

    sub r3, r3, #4
    cmp r3, #4
    bge .packed_strides_vec_loop_4_contig
    b .packed_strides_vec_loop_4_end

    .packed_strides_vec_loop_4:

    vldmia          r1!, { q0, q1 }
    vld1.f32        d8[0], [r5], r6
    vld1.f32        d8[1], [r5], r6
    vld1.f32        d9[0], [r5], r6
    vld1.f32        d9[1], [r5], r6
    vldmia          r1!, { q2, q3 }

    vmla.f32        q8, q0, d8[0]
    vmla.f32        q9, q1, d8[0]
    vldmia          r1!, { q0, q1 }
    vmla.f32        q10, q2, d8[1]
    vmla.f32        q11, q3, d8[1]

    vldmia          r1!, { q2, q3 }

    vmla.f32        q8, q0, d9[0]
    vmla.f32        q9, q1, d9[0]
    vmla.f32        q10, q2, d9[1]
    vmla.f32        q11, q3, d9[1]

    sub r3, r3, #4
    cmp r3, #4
    bge .packed_strides_vec_loop_4

    .packed_strides_vec_loop_4_end:

    vadd.f32        q8, q8, q10
    vadd.f32        q9, q9, q11

    cmp r3, #0
    beq .non_linear
*/

    .packed_strides_vec_loop_1:
    vldmia          r1!, { q0-q3 }
    vld1.f32        d8[0], [r5], r6

    vmla.f32        q8, q0, d8[0]
    vmla.f32        q9, q1, d8[0]
    vldmia          r1!, { q0-q1 }

    vmla.f32        q10, q2, d8[0]
    vmla.f32        q11, q3, d8[0]
    vldmia          r1!, { q2-q3 }

    vmla.f32        q12, q0, d8[0]
    vmla.f32        q13, q1, d8[0]

    vmla.f32        q14, q2, d8[0]
    vmla.f32        q15, q3, d8[0]

    subs r3, r3, #1
    bne .packed_strides_vec
    b   .non_linear

.non_linear:

    ldr     r1, [r0, #16]
    cmp     r1, #0
    bne     .non_linear_loop_entry

.store:
    ldr     r3, [r0, #8]
    ldm     r3, { r4, r5, r6, r7 } // discr, ...
    cmp     r4, #0
    beq     .store_strides
    cmp     r4, #3
    beq     .store_vec_strides

    b       .unsupported

.store_strides:
    // r5:ptr, r6:rsc, r7:csc
    ldr r7, [r3, #32]

.store_vec_strides:
    // r5 ptr, r6: stride r7: item_size
    {% for reg in (16..31) %}
        vst1.f32    d{{reg}}[0], [r5], r6
        vst1.f32    d{{reg}}[1], [r5], r6
    {% endfor %}

    mov         r0,     #0

.return:
    vpop        { q4-q7 }
    pop         { r4-r12 }

    bx          lr

.non_linear_loop_entry:
    sub     r1, #16

.non_linear_loop:
    add     r1, #16
    ldm     r1, { r4, r5, r6, r7 }
    cmp     r4, #0
    beq     .store
    cmp     r4, #1
    beq     .min
    cmp     r4, #2
    beq     .max
    cmp     r4, #3
    beq     .non_linear_addc
    cmp     r4, #4
    beq     .per_row_mul
    cmp     r4, #5
    beq     .per_row_add
    cmp     r4, #6
    beq     .per_col_mul
    cmp     r4, #7
    beq     .per_col_add
    cmp     r4, #8
    beq     .add_row_col_product
    cmp     r4, #9
    beq     .scalar_mul
    cmp     r4, #10
    beq     .scalar_add
    cmp     r4, #14
    beq     .add_unicast

    b .unsupported

.non_linear_addc:
    ldr     r3, [r0, #8]
    ldm     r3, { r4, r5, r6, r7 }
    cmp     r4, #0
    bne     .unsupported

.add_with_strides:
    // r5: ptr, r6:rsc
    {% for reg in (0..15) %}
        vld1.f32    d{{reg}}[0], [ r5 ], r6
        vld1.f32    d{{reg}}[1], [ r5 ], r6
    {% endfor %}
    {% for reg in (0..7) %}
        vadd.f32 q{{reg|plus:8}}, q{{reg|plus:8}}, q{{reg}}
    {% endfor %}

    b .non_linear_loop

.max:
    vmov            s0, r5
    vdup.32         q0, d0[0]
    {% for reg in (8..15) %}
        vmax.f32    q{{reg}}, q{{reg}}, q0
    {% endfor %}

    b .non_linear_loop

.min:
    vmov            s0, r5
    vdup.32         q0, d0[0]
    {% for reg in (8..15) %}
        vmin.f32    q{{reg}}, q{{reg}}, q0
    {% endfor %}
    b .non_linear_loop

.per_row_add:
    mov     r6, #4
    b .add_with_strides

.per_row_mul:
    // r5: ptr
    vldm    r5, { q0-q7 }
    {% for reg in (0..7) %}
        vmul.f32 q{{reg|plus:8}}, q{{reg|plus:8}}, q{{reg}}
    {% endfor %}

    b .non_linear_loop

.per_col_add:
    vld1.f32        d0[0], [ r5 ]
    vdup.32         q0, d0[0]
    {% for reg in (8..15) %}
        vadd.f32    q{{reg}}, q{{reg}}, q0
    {% endfor %}

    b .non_linear_loop

.per_col_mul:
    vld1.f32        d0[0], [ r5 ]
    {% for reg in (8..15) %}
        vmul.f32    q{{reg}}, q{{reg}}, d0[0]
    {% endfor %}

    b .non_linear_loop

.add_row_col_product:
    vld1.f32        d0[0], [ r6 ]
    vldmia          r5!, { q4-q7 }

    vmla.f32        q8, q4, d0[0]
    vmla.f32        q9, q5, d0[0]

    vmla.f32        q10, q6, d0[0]
    vmla.f32        q11, q7, d0[0]

    vldmia          r5!, { q4-q7 }

    vmla.f32        q12, q4, d0[0]
    vmla.f32        q13, q5, d0[0]

    vmla.f32        q14, q6, d0[0]
    vmla.f32        q15, q7, d0[0]

    b .non_linear_loop

.scalar_mul:
    vmov        s0, r5
    vdup.f32    q0, d0[0]

    {% for q in (8..15) %}
        vmul.f32 q{{q}}, q{{q}}, q0
    {% endfor %}

    b .non_linear_loop

.scalar_add:
    vmov        s0, r5
    vdup.f32    q0, d0[0]

    {% for q in (8..15) %}
        vadd.f32 q{{q}}, q{{q}}, q0
    {% endfor %}

    b .non_linear_loop

.add_unicast:
    b .add_with_strides

.unsupported:
    mov         r0,     #1
    b           .return

