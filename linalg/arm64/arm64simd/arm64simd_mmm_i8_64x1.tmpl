// vim: ft=arm

// C tile regs: v16 to v31, no need to preserve

// no preservation either for v0-v7...
// packed A buffering (2x8 values): alternating v0, v1 with v2, v3
// packed B buffering (2x8 values): alternating v4, v5 with v6, v7

.text
.align 4

.cpu generic+fp+simd
.global {{G}}arm64simd_mmm_i8_64x1_{{suffix}}
{{G}}arm64simd_mmm_i8_64x1_{{suffix}}:

/*
    prfm        pldl1keep, [x1]
    prfm        pldl1keep, [x2]
*/

    stp         x20, x21, [sp, #-16]!
    stp         x22, x23, [sp, #-16]!
    stp         x24, x25, [sp, #-16]!
    stp         x26, x27, [sp, #-16]!

{% for r in (16..31) %}
    eor         v{{r}}.8b, v{{r}}.8b, v{{r}}.8b
{% endfor %}

    ldp         x7, x8, [x0]        // a, b
    ldp         x9, x10, [x0, #16]  // c, lin

    ldp         x2, x1, [x7]        // a disc, a first arg

    cmp         x2, #1
    bne         .unsupported

    ldp         x5, x3, [x10]       // lin disc, k
    cmp         x5, #0
    bne         .unsupported
    cmp         x3, #0
    beq         .non_linear

    ldp         x4, x2, [x8]        // b disc, first arg
    cmp         x4, #1
    beq         .packed_packed
    cmp         x4, #2
    beq         .packed_tops_and_offsets
    b           .unsupported

.packed_tops_and_offsets:
    ldr         x8, [x8, #16]   // cols ptr ptr
    ldr         x4, [x2], #8    // fist row offset
    ldr         x5, [x8]        // heads of cols ptr

.packed_tops_and_offsets_loop_1:
    add         x8, x4, x5
    ld1         {v9.b}[0], [ x8 ]
    ldr         x4, [ x2 ], #8
    sshll       v9.8h, v9.8b, 0

    ld1	        { v0.8b-v3.8b }, [ x1 ], #32
    ld1	        { v4.8b-v7.8b }, [ x1 ], #32

    {% for reg in (0..7) %}
        sshll       v10.8h, v{{reg}}.8b, 0
        smlal       v{{reg | times: 2 | plus: 16 }}.4s, v10.4h, v9.h[0]
        smlal2      v{{reg | times: 2 | plus: 17 }}.4s, v10.8h, v9.h[0]
    {% endfor %}

    subs        x3, x3, #1
    bne         .packed_tops_and_offsets_loop_1

    b           .non_linear

.packed_packed:

.packed_packed_loop_1:
    ld1         {v9.b}[0], [ x2 ], 1
    sshll       v9.8h, v9.8b, 0

    ld1	        { v0.8b-v3.8b }, [ x1 ], #32
    ld1	        { v4.8b-v7.8b }, [ x1 ], #32

    {% for reg in (0..7) %}
        sshll       v10.8h, v{{reg}}.8b, 0
        smlal       v{{reg | times: 2 | plus: 16 }}.4s, v10.4h, v9.h[0]
        smlal2      v{{reg | times: 2 | plus: 17 }}.4s, v10.8h, v9.h[0]
    {% endfor %}

    subs        x3, x3, #1
    bne .packed_packed_loop_1

    b .non_linear

.non_linear:
    ldr         x1, [x0, #32]
    cmp         x1, #0
    bne         .non_linear_loop_entry

    cmp         x0, #0
    beq         .store

.store:
    ldr         x3, [x0, #16]
    ldp         x5, x6, [x3]                // c base ptr, rsc
    ldp         x7, x8, [x3, #16]           // csc, item_size

    cmp         x8, #4
    beq         .store_strides_i32

    {% for reg in (16..31) %}
        {% for lane in (0..3) %}
            st1 { v{{reg}}.b }[{{lane | times: 4}}], [ x5 ], x6
        {% endfor %}
    {% endfor %}

    mov         x0, #0
    b           .return

.store_strides_i32:
    {% for reg in (16..31) %}
        {% for lane in (0..3) %}
            st1 { v{{reg}}.s }[{{lane}}], [ x5 ], x6
        {% endfor %}
    {% endfor %}

    mov         x0, #0
    b           .return

.return:
    ldp         x26, x27, [sp], #16
    ldp         x24, x25, [sp], #16
    ldp         x22, x23, [sp], #16
    ldp         x20, x21, [sp], #16

    ret

.non_linear_loop_entry:
    sub         x1, x1, 40

.non_linear_loop:
    add         x1, x1, 40
    ldr         x2, [x1]
    cmp         x2, #0
    beq         .store
    cmp         x2, #1
    beq         .min
    cmp         x2, #2
    beq         .max
    cmp         x2, #3
    beq         .add_unicast
    cmp         x2, #4
    beq         .per_row_mul
    cmp         x2, #5
    beq         .per_row_add
    cmp         x2, #6
    beq         .per_col_mul
    cmp         x2, #7
    beq         .per_col_add
    cmp         x2, #8
    beq         .add_row_col_product
    cmp         x2, #9
    beq         .scalar_mul
    cmp         x2, #10
    beq         .scalar_add
    cmp         x2, #12
    beq         .q_towards_plusinf
    cmp         x2, #13
    beq         .q_away
    cmp         x2, #14
    beq         .q_wrapping_mul_high_doubling
    cmp         x2, #15
    beq         .q_shift_right_rounding

    b           .unsupported

.min:
    add         x2, x1, #8
    ld1r        {v0.4s}, [ x2 ]
    {% for reg in (16..31) %}
        smin        v{{reg}}.4s, v{{reg}}.4s, v0.4s
    {% endfor %}

    b           .non_linear_loop

.max:
    add         x2, x1, #8
    ld1r        {v0.4s}, [ x2 ]
    {% for reg in (16..31) %}
        smax        v{{reg}}.4s, v{{reg}}.4s, v0.4s
    {% endfor %}

    b           .non_linear_loop

.add_unicast:
    ldp         x5, x6, [x1, #8]
    ldp         x7, x8, [x1, #24]

    cmp         x8, #4
    beq         non_linear_addc_i32

    {% for reg in (16..31) %}
        {% for lane in (0..3) %}
            ld1 {v0.b}[{{lane}}], [ x5 ], x6
        {% endfor %}
        sshll v0.8h, v0.8b, 0
        sshll v0.4s, v0.4h, 0
        add v{{reg}}.4s, v{{reg}}.4s, v0.4s
    {% endfor %}

    b           .non_linear_loop

non_linear_addc_i32:
    {% for reg in (16..31) %}
        {% for lane in (0..3) %}
            ld1 {v0.s}[{{lane}}], [ x5 ], x6
        {% endfor %}
        add v{{reg}}.4s, v{{reg}}.4s, v0.4s
    {% endfor %}

    b           .non_linear_loop

.per_col_mul:
    ldr         x2, [x1, #8]
    ld1         {v0.s}[0], [ x2 ]

    {% for reg in (16..31) %}
        mul v{{reg}}.4s, v{{reg}}.4s, v{{0}}.s[0]
    {% endfor %}

    b           .non_linear_loop

.per_col_add:
    ldr         x2, [x1, #8]
    ld1         {v0.s}[0], [ x2 ]
	dup         v0.4s, v0.s[0]

    {% for reg in (16..31) %}
        add v{{reg}}.4s, v{{reg}}.4s, v{{0}}.4s
    {% endfor %}

    b           .non_linear_loop

.per_row_mul:
    ldr         x2, [x1, #8]
    ld1         { v0.4s-v3.4s }, [ x2 ], #64
    ld1         { v4.4s-v7.4s }, [ x2 ], #64
    ld1         { v8.4s-v11.4s }, [ x2 ], #64
    ld1         { v12.4s-v15.4s }, [ x2 ], #64

    {% for reg in (0..15) %}
        mul v{{reg|plus:16}}.4s, v{{reg | plus: 16}}.4s, v{{reg}}.4s
    {% endfor %}

    b           .non_linear_loop

.per_row_add:
    ldr         x2, [x1, #8]
    ld1         { v0.4s-v3.4s }, [ x2 ], #64
    ld1         { v4.4s-v7.4s }, [ x2 ], #64
    ld1         { v8.4s-v11.4s }, [ x2 ], #64
    ld1         { v12.4s-v15.4s }, [ x2 ], #64

    {% for reg in (0..15) %}
        add v{{reg|plus:16}}.4s, v{{reg | plus: 16}}.4s, v{{reg}}.4s
    {% endfor %}

    b           .non_linear_loop

.add_row_col_product:
    ldr     x2, [x1, #8]
    ldr     x3, [x1, #16]

    ld1         { v15.s }[0], [ x3 ]
    xtn         v15.4h, v15.4s

    ld1         { v0.4s-v3.4s }, [ x2 ], #64
    ld1         { v4.4s-v7.4s }, [ x2 ], #64

    {% for reg in (0..7) %}
        xtn         v{{reg}}.4h, v{{reg}}.4s
        smlal        v{{reg|plus: 16}}.4s, v{{reg}}.4h, v15.h[0]
    {% endfor %}

    ld1         { v0.4s-v3.4s }, [ x2 ], #64
    ld1         { v4.4s-v7.4s }, [ x2 ], #64

    {% for reg in (0..7) %}
        xtn         v{{reg}}.4h, v{{reg}}.4s
        smlal        v{{reg|plus: 24}}.4s, v{{reg}}.4h, v15.h[0]
    {% endfor %}

    b           .non_linear_loop

.scalar_mul:
    add         x2, x1, #8
    ld1         {v0.s}[0], [ x2 ]
    {% for reg in (16..31) %}
        mul       v{{reg}}.4s, v{{reg}}.4s, v0.s[0]
    {% endfor %}

    b           .non_linear_loop

.scalar_add:
    add         x2, x1, #8
    ld1         {v0.s}[0], [ x2 ]
    dup         v0.4s, v0.s[0]
    {% for reg in (16..31) %}
        add        v{{reg}}.4s, v{{reg}}.4s, v0.4s
    {% endfor %}

    b           .non_linear_loop

.q_towards_plusinf:
    add         x2, x1, #8
    ld1r        { v0.4s }, [ x2 ]
    add         x2, x1, #16
    ld1r        { v1.4s }, [ x2 ]
    neg         v1.4s, v1.4s

    {% for q in (16..31) %}
        sqrdmulh    v{{q}}.4s, v{{q}}.4s, v0.4s
    {% endfor %}
    {% for q in (16..31) %}
        sqrshl  v{{q}}.4s, v{{q}}.4s, v1.4s
    {% endfor %}

    b .non_linear_loop

.q_away:
    add         x2, x1, #8
    ld1r        { v0.4s }, [ x2 ]
    add         x2, x1, #16
    ld1r        { v1.4s }, [ x2 ]
    neg         v1.4s, v1.4s

    // process data in four chunks, using regs 4 5 6 7 to store sign masks while mul are running
    {% for chunk in (0..3) %}
        {% for q in (0..3) %}
            {% capture sign %}v{{ q | plus:4 }}{% endcapture %}
            {% capture reg %}v{{chunk | times: 4 | plus: 16 | plus: q }}{% endcapture %}
            cmlt        {{sign}}.4s, {{reg}}.4s, #0
            abs         {{reg}}.4s, {{reg}}.4s
            sqdmulh     {{reg}}.4s, {{reg}}.4s, v0.4s
        {% endfor %}
        {% for q in (0..3) %}
            {% capture sign %}v{{ q | plus:4 }}{% endcapture %}
            {% capture reg %}v{{chunk | times: 4 | plus: 16 | plus: q }}{% endcapture %}
            sqrshl      {{reg}}.4s, {{reg}}.4s, v1.4s
            neg         v3.4s, {{reg}}.4s
            bit         {{reg}}.16b, v3.16b, {{sign}}.16b
        {% endfor %}
    {% endfor %}


    b .non_linear_loop

.q_wrapping_mul_high_doubling:
    add         x2, x1, #8
    ld1r        { v0.4s }, [ x2 ]

    {% for q in (16..31) %}
    sqdmulh     v{{q}}.4s, v{{q}}.4s, v0.4s
    {% endfor %}

    b .non_linear_loop

.q_shift_right_rounding:
    ldp         x5, x6, [x1, #8]    // x5: shift, x6: policy

    cmp w5, 0                       // if shift is zero, return 
    beq .non_linear_loop

    mov w3, #1
    ins v4.s[0], w3
    dup v4.4s, v4.4s[0]

    cmp     x6, 1
    beq     .q_shift_right_rounding_zero
    cmp     x6, 2
    beq     .q_shift_right_rounding_away
    cmp     x6, 3
    beq     .q_shift_right_rounding_minus_inf
    cmp     x6, 4
    beq     .q_shift_right_rounding_plus_inf
    cmp     x6, 5
    beq     .q_shift_right_rounding_even
    cmp     x6, 6
    beq     .q_shift_right_rounding_odd
    
    b .unsupported

.q_shift_right_rounding_zero: // signum * (abs >> shift)
    neg w5, w5
    ins v1.s[0], w5
    dup v1.4s, v1.4s[0]

    {% for q in (16..31) %}
        cmlt        v0.4s, v{{q}}.4s, #0
        abs         v{{q}}.4s, v{{q}}.4s
        sqshl      v{{q}}.4s, v{{q}}.4s, v1.4s
        neg         v3.4s, v{{q}}.4s
        bit         v{{q}}.16b, v3.16b, v0.16b
    {% endfor %}
   
    b .non_linear_loop

.q_shift_right_rounding_away: // signum * (abs >> (shift-1) + 1 >> 1)
    sub w5, w5, #1
    neg w5, w5
    ins v1.s[0], w5
    dup v1.4s, v1.4s[0]

    {% for q in (16..31) %}
        cmlt        v0.4s, v{{q}}.4s, #0
        abs         v{{q}}.4s, v{{q}}.4s
        sqshl       v{{q}}.4s, v{{q}}.4s, v1.4s
        add         v{{q}}.4s, v{{q}}.4s, v4.4s
        sshr        v{{q}}.4s, v{{q}}.4s, #1
        neg         v3.4s, v{{q}}.4s
        bit         v{{q}}.16b, v3.16b, v0.16b
    {% endfor %}
   
    b .non_linear_loop

.q_shift_right_rounding_minus_inf: // val >> shift
    neg w5, w5
    ins v1.s[0], w5
    dup v1.4s, v1.4s[0]

    {% for q in (16..31) %}
        sqshl       v{{q}}.4s, v{{q}}.4s, v1.4s
    {% endfor %}
   
    b .non_linear_loop

.q_shift_right_rounding_plus_inf: // (val >> shift-1)+1 >>1
    sub w5, w5, #1
    neg w5, w5
    ins v1.s[0], w5
    dup v1.4s, v1.4s[0]

    {% for q in (16..31) %}
        sqshl       v{{q}}.4s, v{{q}}.4s, v1.4s
        add         v{{q}}.4s, v{{q}}.4s, v4.4s
        sshr        v{{q}}.4s, v{{q}}.4s, #1
    {% endfor %}
   
    b .non_linear_loop

.q_shift_right_rounding_even: // signum * ((abs >> shift-1) + (abs & 0x3 == 0x3) >> 1)
    sub w5, w5, #1
    neg w5, w5
    ins v1.s[0], w5
    dup v1.4s, v1.4s[0]

    {% for q in (16..31) %}
        cmlt        v0.4s, v{{q}}.4s, #0
        abs         v{{q}}.4s, v{{q}}.4s
        sqshl       v{{q}}.4s, v{{q}}.4s, v1.4s

        and         v2.16b, v4.16b, v{{q}}.16b     //val & 0x1 == 0x1
        
        sshr        v3.4s, v{{q}}.4s, #1 
        and         v2.16b, v4.16b, v3.16b         //val & 0x3 == 0x3

        add         v{{q}}.4s, v{{q}}.4s, v2.4s //val += (val & 0x3 == 0x3)

        sshr        v{{q}}.4s, v{{q}}.4s, #1
        neg         v3.4s, v{{q}}.4s
        bit         v{{q}}.16b, v3.16b, v0.16b
    {% endfor %}
   
    b .non_linear_loop

.q_shift_right_rounding_odd: // signum * ((abs >> shift-1) + (abs & 0x3 == 0x1) >> 1)
    sub w5, w5, #1
    neg w5, w5
    ins v1.s[0], w5
    dup v1.4s, v1.4s[0]

    {% for q in (16..31) %}
        cmlt        v0.4s, v{{q}}.4s, #0
        abs         v{{q}}.4s, v{{q}}.4s
        sqshl       v{{q}}.4s, v{{q}}.4s, v1.4s

        and         v2.16b, v4.16b, v{{q}}.16b     //val & 0x1 == 0x1
        
        sshr        v3.4s, v{{q}}.4s, #1 
        not         v3.16b, v3.16b
        and         v2.16b, v4.16b, v3.16b         //val & 0x3 == 0x1

        add         v{{q}}.4s, v{{q}}.4s, v2.4s //val += (val & 0x3 == 0x1)

        sshr        v{{q}}.4s, v{{q}}.4s, #1
        neg         v3.4s, v{{q}}.4s
        bit         v{{q}}.16b, v3.16b, v0.16b
    {% endfor %}
   
    b .non_linear_loop

.unsupported:
    mov         x0, #1
    b           .return
