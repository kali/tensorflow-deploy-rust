// vim: ft=arm

// no preservation either for v0-v7 and v16-v31

.text
.align 4
{% if os == "ios" %}
    .global _arm64simd_ssigmoid_4n
    _arm64simd_ssigmoid_4n:
{% else %}
    .cpu generic+fp+simd
    .global arm64simd_ssigmoid_4n
    arm64simd_ssigmoid_4n:
{% endif %}

    cmp         x1, #0
    beq         .return

    adr         x2, .coeffs_num
    ld1         { v0.4s, v1.4s, v2.4s, v3.4s }, [x2]
    dup         v5.4s, v0.s[0]              // v5 <- low, broadcasted
    dup         v6.4s, v0.s[1]              // v6 <- high, broadcasted
    dup         v7.4s, v3.s[1]              // v7 <- 0.5, broadcasted

.loop:
    ld1         { v16.4s }, [x0]

    fmax        v16.4s, v16.4s, v5.4s
    fmin        v16.4s, v16.4s, v6.4s       // v16 <- x
    fmul        v20.4s, v16.4s, v16.4s      // v20 <- x2

    dup         v24.4s, v0.s[3]
    fmla        v24.4s, v20.4s, v0.s[2]
    dup         v28.4s, v1.s[0]
    fmla        v28.4s, v20.4s, v24.4s
    dup         v24.4s, v1.s[1]
    fmla        v24.4s, v20.4s, v28.4s
    dup         v28.4s, v1.s[2]
    fmla        v28.4s, v20.4s, v24.4s
    fmul        v16.4s, v16.4s, v28.4s      // v16 <- numerator

    dup         v24.4s, v2.s[0]
    fmla        v24.4s, v20.4s, v1.s[3]
    dup         v28.4s, v2.s[1]
    fmla        v28.4s, v20.4s, v24.4s
    dup         v24.4s, v2.s[2]
    fmla        v24.4s, v20.4s, v28.4s
    dup         v28.4s, v2.s[3]
    fmla        v28.4s, v20.4s, v24.4s
    dup         v24.4s, v3.s[0]
    fmla        v24.4s, v20.4s, v28.4s      // v24 <- denum

    fdiv        v16.4s, v16.4s, v24.4s
    fadd        v16.4s, v16.4s, v7.4s

    st1         { v16.4s }, [x0], #16

    subs        x1, x1, #4
    bne         .loop

.return:
    ret

.coeffs_num:
    .float -18.0                    // low          v0
    .float 18.0                     // high         
    .float 4.37031012579801e-11     // alpha_9      
    .float 1.15627324459942e-07     // alpha_7      
    .float 6.08574864600143e-05     // alpha_5      v1
    .float 8.51377133304701e-03     // alpha_3      
    .float 2.48287947061529e-01     // alpha_1      
    .float 6.10247389755681e-13     // beta_10      
    .float 5.76102136993427e-09     // beta_8       v2
    .float 6.29106785017040e-06     // beta_6       
    .float 1.70198817374094e-03     // beta_4       
    .float 1.16817656904453e-01     // beta_2       
    .float 9.93151921023180e-01     // beta_0       v3
    .float 0.5                      //              
    .float 0.0                      // padding
    .float 0.0

/*
    prfm        pldl1keep, [x1]
    prfm        pldl1keep, [x2]

    stp         x19, x20, [sp, #-16]!
    stp         x21, x22, [sp, #-16]!
    stp         x23, x24, [sp, #-16]!
    stp         x25, x26, [sp, #-16]!

{% for r in (16..31) %}
    eor         v{{r}}.8b, v{{r}}.8b, v{{r}}.8b
{% endfor %}

    ldp         x7, x8, [x0]        // a, b
    ldp         x9, x10, [x0, #16]  // c, lin

    ldp         x2, x1, [x7]        // a disc, a first arg

    cmp         x2, #1
    bne         .unsupported

    ldp         x5, x3, [x10]       // lin disc, k
    cmp         x5, #0
    bne         .unsupported
    cmp         x3, #0
    beq         .non_linear

    ldp         x4, x2, [x8]        // b disc, first arg
    cmp         x4, #1
    beq         .packed_packed
    cmp         x4, #2
    beq         .packed_tops_and_offsets
    cmp         x4, #3
    beq         .packed_vec_strides
    b           .unsupported

.packed_tops_and_offsets:
    ldr         x4, [x8, #16]

    ldp         x19, x20, [x4], #16 // heads of cols ptrs
    ldp         x21, x22, [x4], #16
    ldp         x23, x24, [x4], #16
    ldp         x25, x26, [x4], #16

.packed_tops_and_offsets_loop_1:
    ld1         { v0.4s, v1.4s }, [ x1 ], #32

    ldr         x4, [ x2 ], #8

    add         x9, x4, x19
    ld1         {v4.s}[0], [ x9 ]
    add         x10, x4, x20
    ld1         {v4.s}[1], [ x10 ]
    add         x11, x4, x21
    ld1         {v4.s}[2], [ x11 ]
    add         x12, x4, x22
    ld1         {v4.s}[3], [ x12 ]
    add         x13, x4, x23
    ld1         {v5.s}[0], [ x13 ]
    add         x14, x4, x24
    ld1         {v5.s}[1], [ x14 ]
    add         x15, x4, x25
    ld1         {v5.s}[2], [ x15 ]
    add         x9, x4, x26
    ld1         {v5.s}[3], [ x9 ]


    fmla        v16.4s, v0.4s, v4.s[0]
    fmla        v17.4s, v1.4s, v4.s[0]
    fmla        v18.4s, v0.4s, v4.s[1]
    fmla        v19.4s, v1.4s, v4.s[1]
    fmla        v20.4s, v0.4s, v4.s[2]
    fmla        v21.4s, v1.4s, v4.s[2]
    fmla        v22.4s, v0.4s, v4.s[3]
    fmla        v23.4s, v1.4s, v4.s[3]

    fmla        v24.4s, v0.4s, v5.s[0]
    fmla        v25.4s, v1.4s, v5.s[0]
    fmla        v26.4s, v0.4s, v5.s[1]
    fmla        v27.4s, v1.4s, v5.s[1]
    fmla        v28.4s, v0.4s, v5.s[2]
    fmla        v29.4s, v1.4s, v5.s[2]
    fmla        v30.4s, v0.4s, v5.s[3]
    fmla        v31.4s, v1.4s, v5.s[3]

    subs        x3, x3, #1
    bne         .packed_tops_and_offsets_loop_1

    b           .store

.packed_packed:
    cmp         x3, #4
    blt         .packed_packed_loop_1

.packed_packed_loop_4:
    ld1         { v0.4s, v1.4s }, [ x1 ], #32
    ld1         { v4.4s, v5.4s }, [ x2 ], #32

    fmla        v16.4s, v0.4s, v4.s[0]
    fmla        v17.4s, v1.4s, v4.s[0]
    fmla        v18.4s, v0.4s, v4.s[1]
    fmla        v19.4s, v1.4s, v4.s[1]
    fmla        v20.4s, v0.4s, v4.s[2]
    fmla        v21.4s, v1.4s, v4.s[2]
    fmla        v22.4s, v0.4s, v4.s[3]
    fmla        v23.4s, v1.4s, v4.s[3]

    fmla        v24.4s, v0.4s, v5.s[0]
    fmla        v25.4s, v1.4s, v5.s[0]
    fmla        v26.4s, v0.4s, v5.s[1]
    fmla        v27.4s, v1.4s, v5.s[1]
    fmla        v28.4s, v0.4s, v5.s[2]
    fmla        v29.4s, v1.4s, v5.s[2]
    fmla        v30.4s, v0.4s, v5.s[3]
    fmla        v31.4s, v1.4s, v5.s[3]

    ld1         { v2.4s, v3.4s }, [ x1 ], #32
    ld1         { v6.4s, v7.4s }, [ x2 ], #32

    fmla        v16.4s, v2.4s, v6.s[0]
    fmla        v17.4s, v3.4s, v6.s[0]
    fmla        v18.4s, v2.4s, v6.s[1]
    fmla        v19.4s, v3.4s, v6.s[1]
    fmla        v20.4s, v2.4s, v6.s[2]
    fmla        v21.4s, v3.4s, v6.s[2]
    fmla        v22.4s, v2.4s, v6.s[3]
    fmla        v23.4s, v3.4s, v6.s[3]

    fmla        v24.4s, v2.4s, v7.s[0]
    fmla        v25.4s, v3.4s, v7.s[0]
    fmla        v26.4s, v2.4s, v7.s[1]
    fmla        v27.4s, v3.4s, v7.s[1]
    fmla        v28.4s, v2.4s, v7.s[2]
    fmla        v29.4s, v3.4s, v7.s[2]
    fmla        v30.4s, v2.4s, v7.s[3]
    fmla        v31.4s, v3.4s, v7.s[3]

    ld1         { v0.4s, v1.4s }, [ x1 ], #32
    ld1         { v4.4s, v5.4s }, [ x2 ], #32

    fmla        v16.4s, v0.4s, v4.s[0]
    fmla        v17.4s, v1.4s, v4.s[0]
    fmla        v18.4s, v0.4s, v4.s[1]
    fmla        v19.4s, v1.4s, v4.s[1]
    fmla        v20.4s, v0.4s, v4.s[2]
    fmla        v21.4s, v1.4s, v4.s[2]
    fmla        v22.4s, v0.4s, v4.s[3]
    fmla        v23.4s, v1.4s, v4.s[3]

    fmla        v24.4s, v0.4s, v5.s[0]
    fmla        v25.4s, v1.4s, v5.s[0]
    fmla        v26.4s, v0.4s, v5.s[1]
    fmla        v27.4s, v1.4s, v5.s[1]
    fmla        v28.4s, v0.4s, v5.s[2]
    fmla        v29.4s, v1.4s, v5.s[2]
    fmla        v30.4s, v0.4s, v5.s[3]
    fmla        v31.4s, v1.4s, v5.s[3]

    ld1         { v2.4s, v3.4s }, [ x1 ], #32
    ld1         { v6.4s, v7.4s }, [ x2 ], #32

    fmla        v16.4s, v2.4s, v6.s[0]
    fmla        v17.4s, v3.4s, v6.s[0]
    fmla        v18.4s, v2.4s, v6.s[1]
    fmla        v19.4s, v3.4s, v6.s[1]
    fmla        v20.4s, v2.4s, v6.s[2]
    fmla        v21.4s, v3.4s, v6.s[2]
    fmla        v22.4s, v2.4s, v6.s[3]
    fmla        v23.4s, v3.4s, v6.s[3]

    fmla        v24.4s, v2.4s, v7.s[0]
    fmla        v25.4s, v3.4s, v7.s[0]
    fmla        v26.4s, v2.4s, v7.s[1]
    fmla        v27.4s, v3.4s, v7.s[1]
    fmla        v28.4s, v2.4s, v7.s[2]
    fmla        v29.4s, v3.4s, v7.s[2]
    fmla        v30.4s, v2.4s, v7.s[3]
    fmla        v31.4s, v3.4s, v7.s[3]

    sub x3, x3, #4
    cmp x3, #4
    bge .packed_packed_loop_4

    cmp x3, #0
    beq .non_linear

.packed_packed_loop_1:

    ld1         { v0.4s, v1.4s }, [ x1 ], #32
    ld1         { v4.4s, v5.4s }, [ x2 ], #32

    fmla        v16.4s, v0.4s, v4.s[0]
    fmla        v17.4s, v1.4s, v4.s[0]
    fmla        v18.4s, v0.4s, v4.s[1]
    fmla        v19.4s, v1.4s, v4.s[1]
    fmla        v20.4s, v0.4s, v4.s[2]
    fmla        v21.4s, v1.4s, v4.s[2]
    fmla        v22.4s, v0.4s, v4.s[3]
    fmla        v23.4s, v1.4s, v4.s[3]

    fmla        v24.4s, v0.4s, v5.s[0]
    fmla        v25.4s, v1.4s, v5.s[0]
    fmla        v26.4s, v0.4s, v5.s[1]
    fmla        v27.4s, v1.4s, v5.s[1]
    fmla        v28.4s, v0.4s, v5.s[2]
    fmla        v29.4s, v1.4s, v5.s[2]
    fmla        v30.4s, v0.4s, v5.s[3]
    fmla        v31.4s, v1.4s, v5.s[3]

    subs        x3, x3, #1
    bne .packed_packed_loop_1

    b .non_linear

.packed_vec_strides:
    // x2 ->  b ptr
    ldr         x4, [x8, #16]    // b stride

    cmp         x3, #4
    blt         .packed_vec_strides_loop_1

.packed_vec_strides_loop_4:
    ld1         { v0.4s, v1.4s, v2.4s, v3.4s }, [ x1 ], #64
    ld1         { v4.s }[0], [ x2 ], x4
    ld1         { v4.s }[1], [ x2 ], x4
    ld1         { v4.s }[2], [ x2 ], x4
    ld1         { v4.s }[3], [ x2 ], x4

    fmla        v16.4s, v0.4s, v4.s[0]
    fmla        v17.4s, v1.4s, v4.s[0]

    ld1         { v0.4s, v1.4s }, [ x1 ], #32

    fmla        v18.4s, v2.4s, v4.s[1]
    fmla        v19.4s, v3.4s, v4.s[1]

    ld1         { v2.4s, v3.4s }, [ x1 ], #32

    fmla        v16.4s, v0.4s, v4.s[2]
    fmla        v17.4s, v1.4s, v4.s[2]
    fmla        v18.4s, v2.4s, v4.s[3]
    fmla        v19.4s, v3.4s, v4.s[3]

    subs        x3, x3, #4
    cmp         x3, #4
    bge         .packed_vec_strides_loop_4

    fadd        v16.4s, v16.4s, v18.4s
    fadd        v17.4s, v17.4s, v19.4s

    cmp         x3, #0
    beq         .non_linear

.packed_vec_strides_loop_1:

    ld1         { v0.4s, v1.4s }, [ x1 ], #32
    ld1         {v4.s}[0], [ x2 ], x4

    fmla        v16.4s, v0.4s, v4.s[0]
    fmla        v17.4s, v1.4s, v4.s[0]

    subs        x3, x3, #1
    bne         .packed_vec_strides_loop_1

.non_linear:
    ldr         x1, [x0, #32]
    cmp         x1, #0
    bne         .non_linear_loop_entry

    cmp         x0, #0
    beq         .store

.store:
    ldr         x3, [x0, #16]               // c
    ldr         x4, [x3]                    // c disc
    cmp         x4, #0
    beq         .store_strides
    cmp         x4, #3
    beq         .store_vec_strides

.store_strides:
    ldr         x5, [x3, #8]                // c base ptr
    ldr         x6, [x3, #16]               // rsc
    ldr         x7, [x3, #24]               // csc

    {% for col in (8..15) %}
        mov x4, x5
        {% for reg in (0..1) %}
            {% for lane in (0..3) %}
                st1 { v{{col | times:2 | plus: reg}}.s }[{{lane}}], [ x4 ], x6
            {% endfor %}
        {% endfor %}
        add x5, x5, x7
    {% endfor %}

    mov         x0, #0
    b           .return

.store_vec_strides:
    ldr         x5, [x3, #8]                // c base ptr
    ldr         x6, [x3, #16]               // c stride

    {% for reg in (0..1) %}
        {% for lane in (0..3) %}
            st1 { v{{reg| plus:16}}.s }[{{lane}}], [ x5 ], x6
        {% endfor %}
    {% endfor %}

    mov         x0, #0

.return:
    ldp         x25, x26, [sp], #16
    ldp         x23, x24, [sp], #16
    ldp         x21, x22, [sp], #16
    ldp         x19, x20, [sp], #16

    ret

.non_linear_loop_entry:
    sub         x1, x1, 24

.non_linear_loop:
    add         x1, x1, 24
    ldr         x2, [x1]
    cmp         x2, #0
    beq         .store
    cmp         x2, #1
    beq         .min
    cmp         x2, #2
    beq         .max
    cmp         x2, #3
    beq         .non_linear_addc
    cmp         x2, #4
    beq         .per_row_mul
    cmp         x2, #5
    beq         .per_row_add
    cmp         x2, #6
    beq         .per_col_mul
    cmp         x2, #7
    beq         .per_col_add
    cmp         x2, #8
    beq         .add_row_col_product
    cmp         x2, #9
    beq         .scalar_mul
    cmp         x2, #10
    beq         .scalar_add

    b           .unsupported

.min:
    add         x2, x1, #8
    ld1         {v0.s}[0], [ x2 ]
    dup         v0.4s, v0.s[0]
    {% for reg in (16..31) %}
        fmin        v{{reg}}.4s, v{{reg}}.4s, v0.4s
    {% endfor %}

    b           .non_linear_loop

.max:
    add         x2, x1, #8
    ld1         {v0.s}[0], [ x2 ]
    dup         v0.4s, v0.s[0]
    {% for reg in (16..31) %}
        fmax        v{{reg}}.4s, v{{reg}}.4s, v0.4s
    {% endfor %}

    b           .non_linear_loop

.non_linear_addc:
    ldr         x3, [x0, #16]               // c
    ldr         x4, [x3]                    // c disc
    cmp         x4, #0
    bne         .unsupported

    ldr         x5, [x3, #8]                // c base ptr
    ldr         x6, [x3, #16]               // rsc
    ldr         x7, [x3, #24]               // csc

    {% for col in (8..15) %}
        mov x4, x5
        {% for reg in (0..1) %}
            {% for lane in (0..3) %}
                ld1 {v0.s}[{{lane}}], [ x4 ], x6
            {% endfor %}
            fadd v{{col | times:2 | plus: reg}}.4s, v{{col | times:2 | plus: reg}}.4s, v0.4s
        {% endfor %}
        add x5, x5, x7
    {% endfor %}

    b           .non_linear_loop

.per_col_mul:
    ldr         x2, [x1, #8]
    ldr         q0, [ x2 ], #16
    ldr         q1, [ x2 ], #16

    {% for col in (0..7) %}
        {% for reg in (0..1) %}
            fmul v{{col | times:2 | plus: reg|plus:16}}.4s, v{{col | times:2 | plus: reg|plus:16}}.4s, v{{col|divided_by:4}}.s[{{col|modulo:4}}]
        {% endfor %}
    {% endfor %}

    b           .non_linear_loop

.per_col_add:
    ldr         x2, [x1, #8]
    ldr         q0, [ x2 ], #16
    ldr         q1, [ x2 ], #16

    {% for col in (0..7) %}
	dup v2.4s, v{{col|divided_by:4}}.s[{{col|modulo:4}}]
        {% for reg in (0..1) %}
            fadd v{{col | times:2 | plus: reg|plus:16}}.4s, v{{col | times:2 | plus: reg|plus:16}}.4s, v2.4s
	{% endfor %}
    {% endfor %}

    b           .non_linear_loop

.per_row_mul:
    ldr         x2, [x1, #8]
    ldr         q0, [ x2 ], #16
    ldr         q1, [ x2 ], #16

    {% for col in (8..15) %}
        {% for reg in (0..1) %}
            fmul v{{col | times:2 | plus: reg}}.4s, v{{col | times:2 | plus: reg}}.4s, v{{reg}}.4s
        {% endfor %}
    {% endfor %}

    b           .non_linear_loop

.per_row_add:
    ldr         x2, [x1, #8]
    ldr         q0, [ x2 ], #16
    ldr         q1, [ x2 ], #16

    {% for col in (8..15) %}
        {% for reg in (0..1) %}
            fadd v{{col | times:2 | plus: reg}}.4s, v{{col | times:2 | plus: reg}}.4s, v{{reg}}.4s
        {% endfor %}
    {% endfor %}

    b           .non_linear_loop

.add_row_col_product:
    ldr     x2, [x1, #8]
    ldr     x3, [x1, #16]

    ld1         { v0.4s, v1.4s }, [ x2 ], #32
    ld1         { v4.4s, v5.4s }, [ x3 ], #32

    fmla        v16.4s, v0.4s, v4.s[0]
    fmla        v17.4s, v1.4s, v4.s[0]
    fmla        v18.4s, v0.4s, v4.s[1]
    fmla        v19.4s, v1.4s, v4.s[1]
    fmla        v20.4s, v0.4s, v4.s[2]
    fmla        v21.4s, v1.4s, v4.s[2]
    fmla        v22.4s, v0.4s, v4.s[3]
    fmla        v23.4s, v1.4s, v4.s[3]

    fmla        v24.4s, v0.4s, v5.s[0]
    fmla        v25.4s, v1.4s, v5.s[0]
    fmla        v26.4s, v0.4s, v5.s[1]
    fmla        v27.4s, v1.4s, v5.s[1]
    fmla        v28.4s, v0.4s, v5.s[2]
    fmla        v29.4s, v1.4s, v5.s[2]
    fmla        v30.4s, v0.4s, v5.s[3]
    fmla        v31.4s, v1.4s, v5.s[3]

    b           .non_linear_loop

.scalar_mul:
    add         x2, x1, #8
    ld1         {v0.s}[0], [ x2 ]
    dup         v0.4s, v0.s[0]
    {% for reg in (16..31) %}
        fmul        v{{reg}}.4s, v{{reg}}.4s, v0.4s
    {% endfor %}

    b           .non_linear_loop

.scalar_add:
    add         x2, x1, #8
    ld1         {v0.s}[0], [ x2 ]
    dup         v0.4s, v0.s[0]
    {% for reg in (16..31) %}
        fadd        v{{reg}}.4s, v{{reg}}.4s, v0.4s
    {% endfor %}

    b           .non_linear_loop

.unsupported:
    mov         x0, #1
    b           .return
*/
